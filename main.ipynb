{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import the necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81e112f94c2c2665"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Libraries for working with image processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "# Libraries needed to edit/save/watch video clips\n",
    "from moviepy import editor\n",
    "import moviepy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:18:13.621515Z",
     "start_time": "2023-12-03T21:18:13.605406Z"
    }
   },
   "id": "df5b4728f3c5458e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the driver function for our algorithm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bce17e9d13efbd0"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def process_video(test_video, output_video):\n",
    "\t\"\"\"\n",
    "\tRead input video stream and produce a video file with detected lane lines.\n",
    "\tParameters:\n",
    "\t\ttest_video: location of input video file\n",
    "\t\toutput_video: location where output video file is to be saved\n",
    "\t\"\"\"\n",
    "\t# read the video file using VideoFileClip without audio\n",
    "\tinput_video = editor.VideoFileClip(test_video, audio=False)\n",
    "\t# apply the function \"frame_processor\" to each frame of the video\n",
    "\t# will give more detail about \"frame_processor\" in further steps\n",
    "\t# \"processed\" stores the output video\n",
    "\tprocessed = input_video.fl_image(frame_processor)\n",
    "\t# save the output video stream to an mp4 file\n",
    "\tprocessed.write_videofile(output_video, audio=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:15:18.281130Z",
     "start_time": "2023-12-03T21:15:18.253089Z"
    }
   },
   "id": "7a3639f48e5eb583"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define “frame_processor” function where all the processing happens on a frame to detect lane lines."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c721b4af5d4d25e4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def frame_processor(image):\n",
    "\t\"\"\"\n",
    "\tProcess the input frame to detect lane lines.\n",
    "\tParameters:\n",
    "\t\timage: image of a road where one wants to detect lane lines\n",
    "\t\t(we will be passing frames of video to this function)\n",
    "\t\"\"\"\n",
    "\t# convert the RGB image to Gray scale\n",
    "\tgrayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\t# applying gaussian Blur which removes noise from the image \n",
    "\t# and focuses on our region of interest\n",
    "\t# size of gaussian kernel\n",
    "\tkernel_size = 5\n",
    "\t# Applying gaussian blur to remove noise from the frames\n",
    "\tblur = cv2.GaussianBlur(grayscale, (kernel_size, kernel_size), 0)\n",
    "\t# first threshold for the hysteresis procedure\n",
    "\tlow_t = 50\n",
    "\t# second threshold for the hysteresis procedure \n",
    "\thigh_t = 150\n",
    "\t# applying canny edge detection and save edges in a variable\n",
    "\tedges = cv2.Canny(blur, low_t, high_t)\n",
    "\t# since we are getting too many edges from our image, we apply \n",
    "\t# a mask polygon to only focus on the road\n",
    "\t# Will explain Region selection in detail in further steps\n",
    "\tregion = region_selection(edges)\n",
    "\t# Applying hough transform to get straight lines from our image \n",
    "\t# and find the lane lines\n",
    "\t# Will explain Hough Transform in detail in further steps\n",
    "\though = hough_transform(region)\n",
    "\t#lastly we draw the lines on our resulting frame and return it as output \n",
    "\tresult = draw_lane_lines(image, lane_lines(image, hough))\n",
    "\treturn result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:15:40.238154Z",
     "start_time": "2023-12-03T21:15:40.203155Z"
    }
   },
   "id": "fd713fe80822b40f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Region Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aecaf02587bae675"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def region_selection(image):\n",
    "\t\"\"\"\n",
    "\tDetermine and cut the region of interest in the input image.\n",
    "\tParameters:\n",
    "\t\timage: we pass here the output from canny where we have \n",
    "\t\tidentified edges in the frame\n",
    "\t\"\"\"\n",
    "\t# create an array of the same size as of the input image \n",
    "\tmask = np.zeros_like(image) \n",
    "\t# if you pass an image with more then one channel\n",
    "\tif len(image.shape) > 2:\n",
    "\t\tchannel_count = image.shape[2]\n",
    "\t\tignore_mask_color = (255,) * channel_count\n",
    "\t# our image only has one channel so it will go under \"else\"\n",
    "\telse:\n",
    "\t\t# color of the mask polygon (white)\n",
    "\t\tignore_mask_color = 255\n",
    "\t# creating a polygon to focus only on the road in the picture\n",
    "\t# we have created this polygon in accordance to how the camera was placed\n",
    "\trows, cols = image.shape[:2]\n",
    "\tbottom_left = [cols * 0.1, rows * 0.95]\n",
    "\ttop_left\t = [cols * 0.4, rows * 0.6]\n",
    "\tbottom_right = [cols * 0.9, rows * 0.95]\n",
    "\ttop_right = [cols * 0.6, rows * 0.6]\n",
    "\tvertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
    "\t# filling the polygon with white color and generating the final mask\n",
    "\tcv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "\t# performing Bitwise AND on the input image and mask to get only the edges on the road\n",
    "\tmasked_image = cv2.bitwise_and(image, mask)\n",
    "\treturn masked_image\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:15:59.391981Z",
     "start_time": "2023-12-03T21:15:59.368080Z"
    }
   },
   "id": "8eef2fb2d254fde7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will be identifying straight lines in the output image from the above function using Probabilistic Hough Transform"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e74268e3d6f629e5"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def hough_transform(image):\n",
    "\t\"\"\"\n",
    "\tDetermine and cut the region of interest in the input image.\n",
    "\tParameter:\n",
    "\t\timage: grayscale image which should be an output from the edge detector\n",
    "\t\"\"\"\n",
    "\t# Distance resolution of the accumulator in pixels.\n",
    "\trho = 1\t\t\t\n",
    "\t# Angle resolution of the accumulator in radians.\n",
    "\ttheta = np.pi/180\n",
    "\t# Only lines that are greater than threshold will be returned.\n",
    "\tthreshold = 20\t\n",
    "\t# Line segments shorter than that are rejected.\n",
    "\tminLineLength = 20\n",
    "\t# Maximum allowed gap between points on the same line to link them\n",
    "\tmaxLineGap = 500\t\n",
    "\t# function returns an array containing dimensions of straight lines \n",
    "\t# appearing in the input image\n",
    "\treturn cv2.HoughLinesP(image, rho = rho, theta = theta, threshold = threshold,\n",
    "\t\t\t\t\t\tminLineLength = minLineLength, maxLineGap = maxLineGap)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:16:52.105372Z",
     "start_time": "2023-12-03T21:16:52.074585Z"
    }
   },
   "id": "928fa8b769960082"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting Lines on video frames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77f76dde065497e0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def average_slope_intercept(lines):\n",
    "\t\"\"\"\n",
    "\tFind the slope and intercept of the left and right lanes of each image.\n",
    "\tParameters:\n",
    "\t\tlines: output from Hough Transform\n",
    "\t\"\"\"\n",
    "\tleft_lines = [] #(slope, intercept)\n",
    "\tleft_weights = [] #(length,)\n",
    "\tright_lines = [] #(slope, intercept)\n",
    "\tright_weights = [] #(length,)\n",
    "\t\n",
    "\tfor line in lines:\n",
    "\t\tfor x1, y1, x2, y2 in line:\n",
    "\t\t\tif x1 == x2:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# calculating slope of a line\n",
    "\t\t\tslope = (y2 - y1) / (x2 - x1)\n",
    "\t\t\t# calculating intercept of a line\n",
    "\t\t\tintercept = y1 - (slope * x1)\n",
    "\t\t\t# calculating length of a line\n",
    "\t\t\tlength = np.sqrt(((y2 - y1) ** 2) + ((x2 - x1) ** 2))\n",
    "\t\t\t# slope of left lane is negative and for right lane slope is positive\n",
    "\t\t\tif slope < 0:\n",
    "\t\t\t\tleft_lines.append((slope, intercept))\n",
    "\t\t\t\tleft_weights.append((length))\n",
    "\t\t\telse:\n",
    "\t\t\t\tright_lines.append((slope, intercept))\n",
    "\t\t\t\tright_weights.append((length))\n",
    "\t# \n",
    "\tleft_lane = np.dot(left_weights, left_lines) / np.sum(left_weights) if len(left_weights) > 0 else None\n",
    "\tright_lane = np.dot(right_weights, right_lines) / np.sum(right_weights) if len(right_weights) > 0 else None\n",
    "\treturn left_lane, right_lane\n",
    "\n",
    "def pixel_points(y1, y2, line):\n",
    "\t\"\"\"\n",
    "\tConverts the slope and intercept of each line into pixel points.\n",
    "\t\tParameters:\n",
    "\t\t\ty1: y-value of the line's starting point.\n",
    "\t\t\ty2: y-value of the line's end point.\n",
    "\t\t\tline: The slope and intercept of the line.\n",
    "\t\"\"\"\n",
    "\tif line is None:\n",
    "\t\treturn None\n",
    "\tslope, intercept = line\n",
    "\tx1 = int((y1 - intercept)/slope)\n",
    "\tx2 = int((y2 - intercept)/slope)\n",
    "\ty1 = int(y1)\n",
    "\ty2 = int(y2)\n",
    "\treturn ((x1, y1), (x2, y2))\n",
    "\n",
    "def lane_lines(image, lines):\n",
    "\t\"\"\"\n",
    "\tCreate full lenght lines from pixel points.\n",
    "\t\tParameters:\n",
    "\t\t\timage: The input test image.\n",
    "\t\t\tlines: The output lines from Hough Transform.\n",
    "\t\"\"\"\n",
    "\tleft_lane, right_lane = average_slope_intercept(lines)\n",
    "\ty1 = image.shape[0]\n",
    "\ty2 = y1 * 0.6\n",
    "\tleft_line = pixel_points(y1, y2, left_lane)\n",
    "\tright_line = pixel_points(y1, y2, right_lane)\n",
    "\treturn left_line, right_line\n",
    "\n",
    "\t\n",
    "def draw_lane_lines(image, lines, color=[255, 0, 0], thickness=12):\n",
    "\t\"\"\"\n",
    "\tDraw lines onto the input image.\n",
    "\t\tParameters:\n",
    "\t\t\timage: The input test image (video frame in our case).\n",
    "\t\t\tlines: The output lines from Hough Transform.\n",
    "\t\t\tcolor (Default = red): Line color.\n",
    "\t\t\tthickness (Default = 12): Line thickness. \n",
    "\t\"\"\"\n",
    "\tline_image = np.zeros_like(image)\n",
    "\tfor line in lines:\n",
    "\t\tif line is not None:\n",
    "\t\t\tcv2.line(line_image, *line, color, thickness)\n",
    "\treturn cv2.addWeighted(image, 1.0, line_image, 1.0, 0.0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:17:20.015309Z",
     "start_time": "2023-12-03T21:17:19.963456Z"
    }
   },
   "id": "5b5afc9bb08199a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "61ae5a12ca8d27b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
